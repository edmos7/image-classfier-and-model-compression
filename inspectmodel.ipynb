{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e511b5d",
   "metadata": {},
   "source": [
    "# unimib/DSIM 2025-2026: Task 2\n",
    "\n",
    "Plant disease classifier implementation - E. Mosca 925279\n",
    "\n",
    "This notebook serves as a way to check out the base architecture used for model training. The implementation and analysis of its elements follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3955a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573fbfe",
   "metadata": {},
   "source": [
    "The model architecture is quite simple, it is made of 6 CONV2D+BN+RELU+MAXPOOL layers, followed by a linear+relu layer, that is connected to the final layer(with 0.5 dropout during training)\n",
    "\n",
    "6 conv layers were used as that was the required amount to build a receptive field that covered the entire input image(at least 256 in both directions); thats to say the final convolutional feature maps had elements that resulted from processing of the entire input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5d4322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=39, in_features=3, base_filters=16, # conv stride stays 1, kernel size stays 3\n",
    "                 n_conv_layers=6, num_fc_units=256):\n",
    "        super(PlantClassifier, self).__init__()\n",
    "        # want to modularize the conv layers\n",
    "        def make_conv_block(in_channels, out_channels, layer_idx=\"\"):\n",
    "            return nn.Sequential(\n",
    "                collections.OrderedDict([\n",
    "                    ('conv'+layer_idx, nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=\"valid\")),\n",
    "                    ('bn'+layer_idx, nn.BatchNorm2d(out_channels)),\n",
    "                    ('relu'+layer_idx, nn.ReLU(inplace=True)),\n",
    "                    ('pool'+layer_idx, nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "                ])\n",
    "            )\n",
    "        layers = []\n",
    "        current_in_channels = in_features\n",
    "        for i in range(n_conv_layers):\n",
    "            layers.append(make_conv_block(current_in_channels, base_filters * (2 ** i), layer_idx=str(i)))\n",
    "            current_in_channels = base_filters * (2 ** i)\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, in_features, 256, 256)\n",
    "            dummy_output = self.features(dummy_input)\n",
    "            flattened_size = dummy_output.view(1, -1).shape[1]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, num_fc_units),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_fc_units, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f6d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlantClassifier(\n",
      "  (features): Sequential(\n",
      "    (0): Sequential(\n",
      "      (conv0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "      (bn0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu0): ReLU(inplace=True)\n",
      "      (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "      (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu4): ReLU(inplace=True)\n",
      "      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "      (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu5): ReLU(inplace=True)\n",
      "      (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=39, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# visualize model structure\n",
    "print(PlantClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "936ae51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2109351\n",
      "features.0.conv0.weight 432\n",
      "features.0.conv0.bias 16\n",
      "features.0.bn0.weight 16\n",
      "features.0.bn0.bias 16\n",
      "features.1.conv1.weight 4608\n",
      "features.1.conv1.bias 32\n",
      "features.1.bn1.weight 32\n",
      "features.1.bn1.bias 32\n",
      "features.2.conv2.weight 18432\n",
      "features.2.conv2.bias 64\n",
      "features.2.bn2.weight 64\n",
      "features.2.bn2.bias 64\n",
      "features.3.conv3.weight 73728\n",
      "features.3.conv3.bias 128\n",
      "features.3.bn3.weight 128\n",
      "features.3.bn3.bias 128\n",
      "features.4.conv4.weight 294912\n",
      "features.4.conv4.bias 256\n",
      "features.4.bn4.weight 256\n",
      "features.4.bn4.bias 256\n",
      "features.5.conv5.weight 1179648\n",
      "features.5.conv5.bias 512\n",
      "features.5.bn5.weight 512\n",
      "features.5.bn5.bias 512\n",
      "classifier.1.weight 524288\n",
      "classifier.1.bias 256\n",
      "classifier.4.weight 9984\n",
      "classifier.4.bias 39\n"
     ]
    }
   ],
   "source": [
    "# inspect model params\n",
    "model = PlantClassifier()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters: {total_params}')\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a41282e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 16, 127, 127])\n",
      "torch.Size([1, 32, 62, 62])\n",
      "torch.Size([1, 64, 30, 30])\n",
      "torch.Size([1, 128, 14, 14])\n",
      "torch.Size([1, 256, 6, 6])\n",
      "torch.Size([1, 256, 2, 2])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([1, 39])\n"
     ]
    }
   ],
   "source": [
    "#printing image dimensions through convolution\n",
    "x = torch.randn(1, 3, 256, 256)  # Example input tensor\n",
    "print(x.shape)\n",
    "for layer in model.features:\n",
    "    x = layer(x)\n",
    "    print(x.shape)\n",
    "for layer in model.classifier:\n",
    "    x = layer(x)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90e0f8c",
   "metadata": {},
   "source": [
    "### Dev notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d06c9",
   "metadata": {},
   "source": [
    "- Parsimonious model architecture development, having in mind deployment on edge devices \n",
    "- Initial concern is to get a full receptive field, knowing input image size, so that each final output was result of computations on entire input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52c388",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
