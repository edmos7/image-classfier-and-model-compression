{"cells":[{"cell_type":"markdown","id":"1fa34727","metadata":{"id":"1fa34727"},"source":["# unimib/DSIM 2025-2026: Task 2\n","\n","Model Compression - E. Mosca 925279"]},{"cell_type":"code","execution_count":null,"id":"3edb130f","metadata":{"id":"3edb130f"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.utils.prune as prune\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader, Dataset\n","import torch.utils.data as data\n","import time\n","import numpy as np\n","import os\n","\n","\n","from model import PlantClassifier, QuantizedPlantClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0aa86f1d"},"outputs":[],"source":["path_on_gdrive = 'gdrive/My Drive/dsim/Plant_leave_diseases_dataset_without_augmentation.zip'\n","dataset_filename = 'Plant_leave_diseases_dataset_without_augmentation' # do not include the '.zip' extension in the name"],"id":"0aa86f1d"},{"cell_type":"code","source":["from google.colab import drive\n","import zipfile\n","from shutil import copyfile\n","drive.mount('/content/gdrive')\n","copyfile(path_on_gdrive, dataset_filename+'.zip')\n","zipf = zipfile.ZipFile(dataset_filename+'.zip')\n","zipf.extractall()\n","zipf.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pn9jJjyCdZId","outputId":"784e036b-8962-47ab-c42c-02add50f37db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"id":"pn9jJjyCdZId"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4eb1cb7"},"outputs":[],"source":["dataset = ImageFolder(root=dataset_filename, transform=transforms.ToTensor())"],"id":"c4eb1cb7"},{"cell_type":"code","source":["copyfile(\"gdrive/MyDrive/dsim/task2/model.py\", \"model.py\")\n","from model import PlantClassifier, QuantizedPlantClassifier"],"metadata":{"id":"-LCz47ux_C1Y"},"execution_count":null,"outputs":[],"id":"-LCz47ux_C1Y"},{"cell_type":"code","source":["from shutil import copytree\n","copytree(\"gdrive/MyDrive/dsim/task2/models\", \"models\")"],"metadata":{"id":"44IJOkSeFkHA"},"id":"44IJOkSeFkHA","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"3972ec2b","metadata":{"id":"3972ec2b"},"source":["Training of the classifier has finished, now the goal is to reduce its size while maintaining performance so that it might be used in resource-constrained applications such as embedded systems.\n","\n","The base model was trained for 10 epochs, but the best model(with the lowest validation loss) was saved after epoch 9.\n","\n","On the test set, it achieved the following: Test Loss: 0.1145, Test Accuracy: 96.50%\n","\n","Now different techniques will be adopted to maintain classification performance while enhancing speed, and reducing size on memory"]},{"cell_type":"code","execution_count":null,"id":"9527e34b","metadata":{"id":"9527e34b","outputId":"b5c7823b-7c08-4600-a886-42cef6871b61"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"id":"07a06b5e","metadata":{"id":"07a06b5e","outputId":"a6482daf-0078-43a3-98a0-0cc183eb45a3"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["model = PlantClassifier().to(device)\n","model.load_state_dict(torch.load(\"models/best_model.pth\"))"]},{"cell_type":"code","execution_count":null,"id":"5e7d9c04","metadata":{"id":"5e7d9c04","outputId":"421bc57d-a590-4b19-ac62-cff73c98df55"},"outputs":[{"name":"stdout","output_type":"stream","text":["--Best(base) model-- \n","\n","Number of params: 2109351, size in MBs: 8.06987476348877\n"]}],"source":["base_model_nparams = sum([param.numel() for param in model.parameters()])\n","base_model_size = os.path.getsize('models/best_model.pth') / (1024 * 1024)  # size in MB\n","print(\"--Best(base) model--\", \"\\n\")\n","print(f\"Number of params: {base_model_nparams}, size in MBs: {base_model_size}\")"]},{"cell_type":"code","execution_count":null,"id":"3e8a0b8c","metadata":{"id":"3e8a0b8c","outputId":"6f9251b0-4625-489d-897e-d7ea1fb53a82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total parameters: 2109351\n","features.0.conv0.weight 432\n","features.0.conv0.bias 16\n","features.0.bn0.weight 16\n","features.0.bn0.bias 16\n","features.1.conv1.weight 4608\n","features.1.conv1.bias 32\n","features.1.bn1.weight 32\n","features.1.bn1.bias 32\n","features.2.conv2.weight 18432\n","features.2.conv2.bias 64\n","features.2.bn2.weight 64\n","features.2.bn2.bias 64\n","features.3.conv3.weight 73728\n","features.3.conv3.bias 128\n","features.3.bn3.weight 128\n","features.3.bn3.bias 128\n","features.4.conv4.weight 294912\n","features.4.conv4.bias 256\n","features.4.bn4.weight 256\n","features.4.bn4.bias 256\n","features.5.conv5.weight 1179648\n","features.5.conv5.bias 512\n","features.5.bn5.weight 512\n","features.5.bn5.bias 512\n","classifier.1.weight 524288\n","classifier.1.bias 256\n","classifier.4.weight 9984\n","classifier.4.bias 39\n"]}],"source":["total_params = sum(p.numel() for p in model.parameters())\n","print(f'Total parameters: {total_params}')\n","for name, param in model.named_parameters():\n","    print(name, param.numel())"]},{"cell_type":"code","execution_count":null,"id":"284c8492","metadata":{"id":"284c8492","outputId":"fa52478e-1e87-4a36-891f-391d7cfa805b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of layers: 8\n"]}],"source":["model_layers = [module for module in model.modules() if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear)]\n","print(\"Number of layers: \"+str(len(model_layers)))"]},{"cell_type":"markdown","id":"4157aae7","metadata":{"id":"4157aae7"},"source":["### Global Unstructured Pruning"]},{"cell_type":"markdown","id":"4bbf730b","metadata":{"id":"4bbf730b"},"source":["First, an analysis on results of pruning will be done.\n","\n","Pruning does not reduce model file size inherently, but applies a mask over the weights in order to turn some of the latter off by setting them to 0. Speed can be improved\n","\n","The pruning technique that will be used is \"Global Pruning\", and will be done in an unstructured way, that is to say that individual weights will be turned off throughtout all layers of the network.\n","\n","Pruning will be omitted for biases"]},{"cell_type":"code","execution_count":null,"id":"f146a8c4","metadata":{"id":"f146a8c4","outputId":"b62bc45e-d532-4408-cffd-913bfce4fc7c"},"outputs":[{"name":"stdout","output_type":"stream","text":["BEFORE GLOBAL PRUNING:\n","\n","Layer: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 0.00%\n","  Bias   - sparsity: 0.00%\n","Layer: Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 0.00%\n","  Bias   - sparsity: 0.00%\n","Layer: Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 0.00%\n","  Bias   - sparsity: 0.00%\n","Layer: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 0.00%\n","  Bias   - sparsity: 0.00%\n","Layer: Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 0.00%\n","  Bias   - sparsity: 0.00%\n","Layer: Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 0.00%\n","  Bias   - sparsity: 0.00%\n","Layer: Linear(in_features=2048, out_features=256, bias=True)\n","  Weights - sparsity: 0.00%\n","  Bias   - sparsity: 0.00%\n","Layer: Linear(in_features=256, out_features=39, bias=True)\n","  Weights - sparsity: 0.00%\n","  Bias   - sparsity: 0.00%\n"]}],"source":["# Check the model's initial sparsity before pruning.\n","print(\"BEFORE GLOBAL PRUNING:\\n\")\n","\n","for layer in model_layers:\n","    print(f\"Layer: {layer}\")\n","    print(f\"  Weights - sparsity: {100. * float(torch.sum(layer.weight == 0)) / float(layer.weight.nelement()):.2f}%\")\n","    if layer.bias is not None:\n","        print(f\"  Bias   - sparsity: {100. * float(torch.sum(layer.bias == 0)) / float(layer.bias.nelement()):.2f}%\")"]},{"cell_type":"markdown","id":"10be136c","metadata":{"id":"10be136c"},"source":["The model is currently fully dense. Setting weights to 0 will make it more sparse.\n","\n","As a goal, having around 1M params while still keeping the model's performance can be a satisfying achievement, so we will set the proportion of parameters to prune to 50% as the model has currently around 2M params."]},{"cell_type":"code","execution_count":null,"id":"ded41e71","metadata":{"id":"ded41e71"},"outputs":[],"source":["parameters_to_prune = [(layer, 'weight') for layer in model_layers]\n","prune.global_unstructured(\n","    parameters_to_prune,\n","    pruning_method=prune.L1Unstructured,\n","    amount=0.5,\n",")"]},{"cell_type":"code","execution_count":null,"id":"42c13b56","metadata":{"id":"42c13b56","outputId":"45726a7a-4e75-4bc1-c67c-884441519a5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Layer: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 21.30%\n","Layer: Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 37.00%\n","Layer: Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 40.00%\n","Layer: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 41.35%\n","Layer: Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 40.84%\n","Layer: Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 39.80%\n","Layer: Linear(in_features=2048, out_features=256, bias=True)\n","  Weights - sparsity: 79.53%\n","Layer: Linear(in_features=256, out_features=39, bias=True)\n","  Weights - sparsity: 64.24%\n"]}],"source":["# after global unstructured pruning\n","for layer in model_layers:\n","    print(f\"Layer: {layer}\")\n","    print(f\"  Weights - sparsity: {100. * float(torch.sum(layer.weight == 0)) / float(layer.weight.nelement()):.2f}%\")"]},{"cell_type":"markdown","id":"5a7f3596","metadata":{"id":"5a7f3596"},"source":["Its noticeable that the layers most affected (on the basis of L1 criterion, so absolute value) are the final linear fully-connected ones, this is good also because they are the heaviest parts of themodel along with the 5th convolution.\n","\n","The middle Conv layers are also more affected than the initial conv layers, this might be because by the time the input reaches them, the size of the feature maps is smaller, so each filterelement will have less infomation to absorb."]},{"cell_type":"code","execution_count":null,"id":"5197dffa","metadata":{"id":"5197dffa"},"outputs":[],"source":["# with the following code pruning can be made permanent\n","for module, param_name in parameters_to_prune:\n","    prune.remove(module, param_name)"]},{"cell_type":"code","execution_count":null,"id":"9aeba941","metadata":{"id":"9aeba941","outputId":"f3f79f32-38dc-4e71-b383-c1f948d67245"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of non-zero parameters after pruning: 1053016\n"]}],"source":["# how many non-zero parameters left?\n","n_nonzero_params = sum(int(torch.sum(layer.weight != 0)) for layer in model_layers)\n","print(f'Number of non-zero parameters after pruning: {n_nonzero_params}')"]},{"cell_type":"markdown","id":"12125d2b","metadata":{"id":"12125d2b"},"source":["Now, immediate performance changes can be analyzed by using this model on the test set"]},{"cell_type":"code","execution_count":null,"id":"f2ff77b1","metadata":{"id":"f2ff77b1"},"outputs":[],"source":["### From previous notebooks, splits done with same seeds...###\n","\n"," Stratified split into train (80%), val (10%), test (10%)\n","from sklearn.model_selection import train_test_split\n","\n","labels = [sample[1] for sample in dataset.samples]\n","\n","#80% train, 20% temp\n","train_indices, temp_indices = train_test_split(\n","    range(len(dataset)),\n","    test_size=0.2,\n","    stratify=labels,\n","    random_state=42\n",")\n","\n","# split the 20% into 50-50 for val (10%) and test (10%)\n","temp_labels = [labels[i] for i in temp_indices]\n","val_indices, test_indices = train_test_split(\n","    temp_indices,\n","    test_size=0.5,\n","    stratify=temp_labels,\n","    random_state=42\n",")\n","\n","train_dataset = data.Subset(dataset, train_indices)\n","val_dataset = data.Subset(dataset, val_indices)\n","test_dataset = data.Subset(dataset, test_indices)\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.RandomRotation(15),\n","    transforms.RandomVerticalFlip(p=0.25),\n","    transforms.RandomHorizontalFlip(p=0.25)\n","])\n","val_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor()\n","])\n","\n","train_dataset.dataset.transform = train_transform\n","val_dataset.dataset.transform = val_transform\n","test_dataset.dataset.transform = val_transform"]},{"cell_type":"markdown","id":"ea6cfd53","metadata":{"id":"ea6cfd53"},"source":[]},{"cell_type":"code","execution_count":null,"id":"a1940ba3","metadata":{"id":"a1940ba3"},"outputs":[],"source":["batch_size = 32\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"id":"72a28c49","metadata":{"id":"72a28c49","outputId":"ca3d6ee0-f655-403e-c82e-1b0485c747b3"},"outputs":[{"data":{"text/plain":["5545"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["len(test_dataset)"]},{"cell_type":"code","execution_count":null,"id":"6eea953a","metadata":{"id":"6eea953a"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"id":"6d26b2fa","metadata":{"id":"6d26b2fa","outputId":"d32dcf75-80e5-4a5b-bc09-7cce844338fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pruned model test time: 63.33 seconds\n","Test Loss: 0.2371, Test Accuracy: 92.84%\n"]}],"source":["test_accuracies = []\n","test_losses = []\n","batch_sizes = []\n","model.eval()\n","with torch.no_grad():\n","    test_time_start = time.time()\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        test_losses.append(loss_fn(outputs, labels).item())\n","        test_accuracies.append((outputs.argmax(dim=1) == labels).float().mean().item())\n","        batch_sizes.append(len(labels))\n","\n","pruned_model_test_time = time.time() - test_time_start\n","print(f\"Pruned model test time: {pruned_model_test_time:.2f} seconds\")\n","test_accuracies_weighted = [acc * size for acc, size in zip(test_accuracies, batch_sizes)]\n","test_losses_weighted = [loss * size for loss, size in zip(test_losses, batch_sizes)]\n","print(f'Test Loss: {sum(test_losses_weighted)/sum(batch_sizes):.4f}, Test Accuracy: {sum(test_accuracies_weighted)/sum(batch_sizes)*100:.2f}%')"]},{"cell_type":"markdown","id":"c39ca8b7","metadata":{"id":"c39ca8b7"},"source":["The model encounters a reduction in accuracy of almost 4%, while the loss on the test set doubled.\n","\n","While this still isn't too bad on 39 classes, it can be improved by fine-tuning the new architecture."]},{"cell_type":"code","execution_count":null,"id":"59450ab5","metadata":{"id":"59450ab5"},"outputs":[],"source":["#torch.save(model.state_dict(), './models/pruned_model.pth')"]},{"cell_type":"code","execution_count":null,"id":"33471c82","metadata":{"id":"33471c82","outputId":"b055899c-95ce-466e-ed2f-f33122aab030"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["pruned_model = PlantClassifier().to(device)\n","pruned_model.load_state_dict(torch.load(\"models/pruned_model.pth\"))"]},{"cell_type":"code","execution_count":null,"id":"03c1f825","metadata":{"id":"03c1f825","outputId":"d58dddc9-2e84-4e1e-df02-4d14f06d6bc9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pruned model size in MBs: 8.06916332244873\n"]}],"source":["#pruned model size... is same as initial since we are just zeroing weights\n","pruned_model_size = os.path.getsize('models/pruned_model.pth') / (1024 * 1024)  # size in MB\n","print(f'Pruned model size in MBs: {pruned_model_size}')"]},{"cell_type":"code","execution_count":null,"id":"c9586779","metadata":{"id":"c9586779"},"outputs":[],"source":["num_ft_epochs = 2\n","learning_rate = 0.001\n","optimizer = torch.optim.Adam(pruned_model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"id":"d26d44bd","metadata":{"id":"d26d44bd"},"outputs":[],"source":["val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"id":"3a88a90a","metadata":{"id":"3a88a90a","outputId":"e567a11a-49e0-4190-ddd1-e87446d4a1a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finetune Epoch [1/2], Loss: 0.0720\n","Finetune Epoch [2/2], Loss: 0.0125\n"]}],"source":["# finetuning the pruned model must be done while keeping the 0-weights frozen\n","pruned_model_layers = [module for module in pruned_model.modules() if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear)]\n","for layer in pruned_model_layers:\n","    mask = layer.weight.data != 0 # setting up mask, so that incoming gradients to 0-weights are also zeroed\n","    layer.weight.register_hook(lambda grad, mask=mask: grad * mask.float())\n","\n","for epoch in range(num_ft_epochs):\n","    pruned_model.train()\n","    for images, labels in val_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = pruned_model(images)\n","        loss = loss_fn(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","    print(f'Finetune Epoch [{epoch+1}/{num_ft_epochs}], Loss: {loss.item():.4f}')"]},{"cell_type":"markdown","source":["**Note**: fine-tuning on the *train* set instead of the validation set would probably have been the more responsible choice here..."],"metadata":{"id":"IO8eGOxmzlrV"},"id":"IO8eGOxmzlrV"},{"cell_type":"code","execution_count":null,"id":"dc02cd9a","metadata":{"id":"dc02cd9a","outputId":"6c59a4a9-1ffd-493f-c36f-06d265dff4b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Layer: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 21.30%\n","Layer: Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 37.00%\n","Layer: Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 40.00%\n","Layer: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 41.35%\n","Layer: Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 40.84%\n","Layer: Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n","  Weights - sparsity: 39.80%\n","Layer: Linear(in_features=2048, out_features=256, bias=True)\n","  Weights - sparsity: 79.53%\n","Layer: Linear(in_features=256, out_features=39, bias=True)\n","  Weights - sparsity: 64.24%\n"]}],"source":["#check if sparsity was preserved\n","for layer in pruned_model_layers:\n","    print(f\"Layer: {layer}\")\n","    print(f\"  Weights - sparsity: {100. * float(torch.sum(layer.weight == 0)) / float(layer.weight.nelement()):.2f}%\")"]},{"cell_type":"code","execution_count":null,"id":"f13174af","metadata":{"id":"f13174af","outputId":"843ea975-fb21-4103-c593-eff72a709475"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of non-zero parameters after finetuning: 1053016\n"]}],"source":["#check if number of non-zero params is same\n","n_nonzero_params_after_ft = sum(int(torch.sum(layer.weight != 0)) for layer in pruned_model_layers)\n","print(f'Number of non-zero parameters after finetuning: {n_nonzero_params_after_ft}')"]},{"cell_type":"code","execution_count":null,"id":"9fe132b9","metadata":{"id":"9fe132b9","outputId":"a3e43e7c-9fd2-4f59-c8eb-9de29a7a7478"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pruned model test time: 17.28 seconds\n","Test Loss: 0.0666, Test Accuracy: 97.69%\n"]}],"source":["# now retrying on the test set\n","test_accuracies = []\n","test_losses = []\n","batch_sizes = []\n","pruned_model.eval()\n","with torch.no_grad():\n","    test_time_start = time.time()\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = pruned_model(images)\n","        test_losses.append(loss_fn(outputs, labels).item())\n","        test_accuracies.append((outputs.argmax(dim=1) == labels).float().mean().item())\n","        batch_sizes.append(len(labels))\n","pruned_model_test_time = time.time() - test_time_start\n","print(f\"Pruned model test time: {pruned_model_test_time:.2f} seconds\")\n","test_accuracies_weighted = [acc * size for acc, size in zip(test_accuracies, batch_sizes)]\n","test_losses_weighted = [loss * size for loss, size in zip(test_losses, batch_sizes)]\n","print(f'Test Loss: {sum(test_losses_weighted)/sum(batch_sizes):.4f}, Test Accuracy: {sum(test_accuracies_weighted)/sum(batch_sizes)*100:.2f}%')"]},{"cell_type":"markdown","id":"6f41cf7b","metadata":{"id":"6f41cf7b"},"source":["Reasonably, fine tuning the pruned model for 2 epochs on the validation set results in slightly better performance in both loss and accuracy on the test set."]},{"cell_type":"code","execution_count":null,"id":"fed3f9cc","metadata":{"id":"fed3f9cc"},"outputs":[],"source":["#torch.save(pruned_model.state_dict(), './models/pruned_finetuned_model.pth')"]},{"cell_type":"markdown","id":"17b45609","metadata":{"id":"17b45609"},"source":["### Quantization"]},{"cell_type":"markdown","id":"2b0688b4","metadata":{"id":"2b0688b4"},"source":["Quantization can help reduce model file size by going from full precision weights(float32) to integers like uint8. This will be done both on the best-base model and the pruned ones in order to compare performance and speed."]},{"cell_type":"code","execution_count":null,"id":"683a8dd1","metadata":{"id":"683a8dd1","outputId":"8462866a-737e-4ab7-cca3-2d47d2ca00a7"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["q_model = QuantizedPlantClassifier() # this is just plant classifier with added quantstubs\n","q_model.load_state_dict(torch.load(\"models/best_model.pth\"))"]},{"cell_type":"code","execution_count":null,"id":"4be0b7db","metadata":{"id":"4be0b7db","outputId":"c083346f-fb37-48b1-f6ee-772a84ba8d6a"},"outputs":[{"data":{"text/plain":["QuantizedPlantClassifier(\n","  (quant): QuantStub()\n","  (features): Sequential(\n","    (0): Sequential(\n","      (conv0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n","      (bn0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (1): Sequential(\n","      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n","      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (2): Sequential(\n","      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (3): Sequential(\n","      (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n","      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu3): ReLU(inplace=True)\n","      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (4): Sequential(\n","      (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n","      (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu4): ReLU(inplace=True)\n","      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (5): Sequential(\n","      (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n","      (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu5): ReLU(inplace=True)\n","      (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): Linear(in_features=2048, out_features=256, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=256, out_features=39, bias=True)\n","  )\n","  (dequant): DeQuantStub()\n",")"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["q_model"]},{"cell_type":"markdown","id":"3f2f3cfa","metadata":{"id":"3f2f3cfa"},"source":["The quantization technique applied will be static quantization and the target dtype will be uint8; doing this in PyTorch allows for quantizing conv layers as well.\n","\n","Static quantization involves a calibration step where representative data is ran through the network. The layer weights can be scaled into uint8 range (-128 to 127) simply by finding a scale factor (divide max abs value weight by 127), so this problem is solved simply by having the pretrained model's weights.\n","\n","Static quantization though requires a calibration step as well, since we can't find a scale to use for activation values from the weights alone(these depend on input data)."]},{"cell_type":"code","execution_count":null,"id":"ebd72dc3","metadata":{"id":"ebd72dc3","outputId":"2f972ec1-74a9-4d8a-84b1-619e9b41258b"},"outputs":[{"name":"stdout","output_type":"stream","text":["fbgemm\n"]}],"source":["print(torch.backends.quantized.engine)"]},{"cell_type":"markdown","id":"bf591efa","metadata":{"id":"bf591efa"},"source":["Quantization models raise a few errors on pytorch. For these reasons the backends.quantized.engine is gonna change to fbgemm"]},{"cell_type":"code","execution_count":null,"id":"0f1826f6","metadata":{"id":"0f1826f6"},"outputs":[],"source":["torch.backends.quantized.engine = 'fbgemm' #encountered issues on x86"]},{"cell_type":"code","execution_count":null,"id":"872e937e","metadata":{"id":"872e937e","outputId":"f4ab5b75-a529-45f2-e1cb-a1208beab223"},"outputs":[{"data":{"text/plain":["QuantizedPlantClassifier(\n","  (quant): QuantStub()\n","  (features): Sequential(\n","    (0): Sequential(\n","      (conv0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n","      (bn0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (1): Sequential(\n","      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n","      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (2): Sequential(\n","      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (3): Sequential(\n","      (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n","      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu3): ReLU(inplace=True)\n","      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (4): Sequential(\n","      (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n","      (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu4): ReLU(inplace=True)\n","      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (5): Sequential(\n","      (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n","      (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu5): ReLU(inplace=True)\n","      (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): Linear(in_features=2048, out_features=256, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=256, out_features=39, bias=True)\n","  )\n","  (dequant): DeQuantStub()\n",")"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["q_model.eval()"]},{"cell_type":"code","execution_count":null,"id":"a808ce4c","metadata":{"id":"a808ce4c"},"outputs":[],"source":["# fuse modules\n","def fuse_model(model):\n","    \"\"\"Fuse model modules for quantization.\"\"\"\n","    # For models with Sequential blocks, you need to build fusion lists\n","    fusion_list = []\n","\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Sequential):\n","            # Build fusion patterns for this Sequential block\n","            for idx in range(len(module) - 1):\n","                if isinstance(module[idx], nn.Conv2d) and isinstance(module[idx + 1], nn.ReLU):\n","                    fusion_list.append([f\"{name}.{idx}\", f\"{name}.{idx + 1}\"])\n","                if isinstance(module[idx], nn.Linear) and isinstance(module[idx + 1], nn.ReLU):\n","                    fusion_list.append([f\"{name}.{idx}\", f\"{name}.{idx + 1}\"])\n","\n","    # Fuse the layers\n","    torch.quantization.fuse_modules(model, fusion_list, inplace=True)\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"c8c7a948","metadata":{"id":"c8c7a948"},"outputs":[],"source":["q_model = fuse_model(q_model)"]},{"cell_type":"code","execution_count":null,"id":"e95f9f3f","metadata":{"id":"e95f9f3f"},"outputs":[],"source":["# Attaching observer modules, activation value mins and maxes will be recorded\n","q_model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")"]},{"cell_type":"code","execution_count":null,"id":"18a5548e","metadata":{"id":"18a5548e","outputId":"9f4515d5-c5d9-440c-e766-dec59eb2be8c"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\user2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\ao\\quantization\\observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n","  warnings.warn(\n"]},{"data":{"text/plain":["QuantizedPlantClassifier(\n","  (quant): QuantStub(\n","    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","  )\n","  (features): Sequential(\n","    (0): Sequential(\n","      (conv0): Conv2d(\n","        3, 16, kernel_size=(3, 3), stride=(1, 1)\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (bn0): BatchNorm2d(\n","        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (1): Sequential(\n","      (conv1): Conv2d(\n","        16, 32, kernel_size=(3, 3), stride=(1, 1)\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (bn1): BatchNorm2d(\n","        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (relu1): ReLU(inplace=True)\n","      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (2): Sequential(\n","      (conv2): Conv2d(\n","        32, 64, kernel_size=(3, 3), stride=(1, 1)\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (bn2): BatchNorm2d(\n","        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (relu2): ReLU(inplace=True)\n","      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (3): Sequential(\n","      (conv3): Conv2d(\n","        64, 128, kernel_size=(3, 3), stride=(1, 1)\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (bn3): BatchNorm2d(\n","        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (relu3): ReLU(inplace=True)\n","      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (4): Sequential(\n","      (conv4): Conv2d(\n","        128, 256, kernel_size=(3, 3), stride=(1, 1)\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (bn4): BatchNorm2d(\n","        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (relu4): ReLU(inplace=True)\n","      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (5): Sequential(\n","      (conv5): Conv2d(\n","        256, 512, kernel_size=(3, 3), stride=(1, 1)\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (bn5): BatchNorm2d(\n","        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (relu5): ReLU(inplace=True)\n","      (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): LinearReLU(\n","      (0): Linear(in_features=2048, out_features=256, bias=True)\n","      (1): ReLU(inplace=True)\n","      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","    )\n","    (2): Identity()\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(\n","      in_features=256, out_features=39, bias=True\n","      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","    )\n","  )\n","  (dequant): DeQuantStub()\n",")"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["torch.quantization.prepare(q_model, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"b36404da","metadata":{"id":"b36404da"},"outputs":[],"source":["# calibration done on val dataset, as best_model was never trained on it\n","with torch.no_grad():\n","    for images, labels in val_loader:\n","        images = images.to(\"cpu\") # done on cpu, as quantized models dont run on gpu, this model isnt quantized yet but this avoids some errors\n","        q_model(images)"]},{"cell_type":"markdown","source":["**Note**: the choice of calibrating on the *validation* set here stems from the fact that calibration should be done on test-representative data, validation set therefore was used as model was already trained on train data and test data will be used for evaluation."],"metadata":{"id":"a18cp87H0aU3"},"id":"a18cp87H0aU3"},{"cell_type":"code","execution_count":null,"id":"95abb827","metadata":{"id":"95abb827","outputId":"ba3fddfd-c856-4715-84c7-d8240e6bc5a6"},"outputs":[{"data":{"text/plain":["QuantizedPlantClassifier(\n","  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n","  (features): Sequential(\n","    (0): Sequential(\n","      (conv0): QuantizedConv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.040684640407562256, zero_point=48)\n","      (bn0): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (1): Sequential(\n","      (conv1): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.3340418040752411, zero_point=75)\n","      (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (2): Sequential(\n","      (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.439125657081604, zero_point=75)\n","      (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (3): Sequential(\n","      (conv3): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.7613955736160278, zero_point=76)\n","      (bn3): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu3): ReLU(inplace=True)\n","      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (4): Sequential(\n","      (conv4): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.309577226638794, zero_point=74)\n","      (bn4): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu4): ReLU(inplace=True)\n","      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (5): Sequential(\n","      (conv5): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.5241798162460327, zero_point=79)\n","      (bn5): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu5): ReLU(inplace=True)\n","      (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): QuantizedLinearReLU(in_features=2048, out_features=256, scale=0.24760189652442932, zero_point=0, qscheme=torch.per_channel_affine)\n","    (2): Identity()\n","    (3): QuantizedDropout(p=0.5, inplace=False)\n","    (4): QuantizedLinear(in_features=256, out_features=39, scale=1.1836150884628296, zero_point=91, qscheme=torch.per_channel_affine)\n","  )\n","  (dequant): DeQuantize()\n",")"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["torch.quantization.convert(q_model, inplace=True) #now model is quantized"]},{"cell_type":"markdown","id":"d82edf7f","metadata":{"id":"d82edf7f"},"source":["As can be seen above, layers have been converted to their quantized counterpart, and the dtype is now uint8 all throughout"]},{"cell_type":"code","execution_count":null,"id":"d3ea66ed","metadata":{"id":"d3ea66ed"},"outputs":[],"source":["#torch.save(q_model.state_dict(), './models/_fbgemm_quantized_model_dict.pth')"]},{"cell_type":"code","execution_count":null,"id":"418b6c01","metadata":{"id":"418b6c01","outputId":"bf157e48-a813-418e-aa73-0cff01865fc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Quantized model size in MBs: 2.077643394470215\n"]}],"source":["#quantized model size\n","quantized_model_size = os.path.getsize('models/_fbgemm_quantized_model_dict.pth') / (1024 * 1024)  # size in MB\n","print(f'Quantized model size in MBs: {quantized_model_size}')"]},{"cell_type":"markdown","id":"a7c3a17a","metadata":{"id":"a7c3a17a"},"source":["The model size was reduced by around 6MBs"]},{"cell_type":"code","execution_count":null,"id":"cbe65264","metadata":{"id":"cbe65264","outputId":"eb076809-8db0-4d88-e682-fe56e0c83191"},"outputs":[{"name":"stdout","output_type":"stream","text":["fbgemm\n"]}],"source":["print(torch.backends.quantized.engine)"]},{"cell_type":"code","execution_count":null,"id":"22f2cea8","metadata":{"id":"22f2cea8","outputId":"e9635898-ec14-4fe6-e04b-7125b21ecab9"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Weight dtypes after static conversion ---\n",": <class 'model.QuantizedPlantClassifier'>\n","quant: <class 'torch.ao.nn.quantized.modules.Quantize'>\n","features: <class 'torch.nn.modules.container.Sequential'>\n","features.0: <class 'torch.nn.modules.container.Sequential'>\n","features.0.conv0: <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n","features.0.bn0: <class 'torch.ao.nn.quantized.modules.batchnorm.BatchNorm2d'>\n","features.0.relu0: <class 'torch.nn.modules.activation.ReLU'>\n","features.0.pool0: <class 'torch.nn.modules.pooling.MaxPool2d'>\n","features.1: <class 'torch.nn.modules.container.Sequential'>\n","features.1.conv1: <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n","features.1.bn1: <class 'torch.ao.nn.quantized.modules.batchnorm.BatchNorm2d'>\n","features.1.relu1: <class 'torch.nn.modules.activation.ReLU'>\n","features.1.pool1: <class 'torch.nn.modules.pooling.MaxPool2d'>\n","features.2: <class 'torch.nn.modules.container.Sequential'>\n","features.2.conv2: <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n","features.2.bn2: <class 'torch.ao.nn.quantized.modules.batchnorm.BatchNorm2d'>\n","features.2.relu2: <class 'torch.nn.modules.activation.ReLU'>\n","features.2.pool2: <class 'torch.nn.modules.pooling.MaxPool2d'>\n","features.3: <class 'torch.nn.modules.container.Sequential'>\n","features.3.conv3: <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n","features.3.bn3: <class 'torch.ao.nn.quantized.modules.batchnorm.BatchNorm2d'>\n","features.3.relu3: <class 'torch.nn.modules.activation.ReLU'>\n","features.3.pool3: <class 'torch.nn.modules.pooling.MaxPool2d'>\n","features.4: <class 'torch.nn.modules.container.Sequential'>\n","features.4.conv4: <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n","features.4.bn4: <class 'torch.ao.nn.quantized.modules.batchnorm.BatchNorm2d'>\n","features.4.relu4: <class 'torch.nn.modules.activation.ReLU'>\n","features.4.pool4: <class 'torch.nn.modules.pooling.MaxPool2d'>\n","features.5: <class 'torch.nn.modules.container.Sequential'>\n","features.5.conv5: <class 'torch.ao.nn.quantized.modules.conv.Conv2d'>\n","features.5.bn5: <class 'torch.ao.nn.quantized.modules.batchnorm.BatchNorm2d'>\n","features.5.relu5: <class 'torch.nn.modules.activation.ReLU'>\n","features.5.pool5: <class 'torch.nn.modules.pooling.MaxPool2d'>\n","classifier: <class 'torch.nn.modules.container.Sequential'>\n","classifier.0: <class 'torch.nn.modules.flatten.Flatten'>\n","classifier.1: <class 'torch.ao.nn.intrinsic.quantized.modules.linear_relu.LinearReLU'>\n","classifier.1._packed_params: <class 'torch.ao.nn.quantized.modules.linear.LinearPackedParams'>\n","classifier.2: <class 'torch.nn.modules.linear.Identity'>\n","classifier.3: <class 'torch.ao.nn.quantized.modules.dropout.Dropout'>\n","classifier.4: <class 'torch.ao.nn.quantized.modules.linear.Linear'>\n","classifier.4._packed_params: <class 'torch.ao.nn.quantized.modules.linear.LinearPackedParams'>\n","dequant: <class 'torch.ao.nn.quantized.modules.DeQuantize'>\n"]}],"source":["print(\"--- Weight dtypes after static conversion ---\") # layers are named, but are Quantized versions\n","# Iterate through the quantized model's layers\n","for name, module in q_model.named_modules():\n","    print(f\"{name}: {type(module)}\")"]},{"cell_type":"code","execution_count":null,"id":"ef1ca0cb","metadata":{"id":"ef1ca0cb","outputId":"b3a8aac0-1179-4ef9-d8e9-50d68eb739a6"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\user2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\ao\\quantization\\observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n","  warnings.warn(\n","C:\\Users\\user2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\ao\\quantization\\observer.py:1209: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n","  warnings.warn(\n"]},{"data":{"text/plain":["QuantizedPlantClassifier(\n","  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n","  (features): Sequential(\n","    (0): Sequential(\n","      (conv0): QuantizedConv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)\n","      (bn0): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (1): Sequential(\n","      (conv1): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)\n","      (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (2): Sequential(\n","      (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)\n","      (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (3): Sequential(\n","      (conv3): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)\n","      (bn3): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu3): ReLU(inplace=True)\n","      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (4): Sequential(\n","      (conv4): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)\n","      (bn4): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu4): ReLU(inplace=True)\n","      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (5): Sequential(\n","      (conv5): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)\n","      (bn5): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu5): ReLU(inplace=True)\n","      (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): QuantizedLinear(in_features=2048, out_features=256, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n","    (2): ReLU(inplace=True)\n","    (3): QuantizedDropout(p=0.5, inplace=False)\n","    (4): QuantizedLinear(in_features=256, out_features=39, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n","  )\n","  (dequant): DeQuantize()\n",")"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["# LOADING QUANTIZED\n","q_model = QuantizedPlantClassifier()\n","q_model.cpu()\n","q_model.eval()\n","q_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n","torch.quantization.prepare(q_model, inplace=True)\n","torch.quantization.convert(q_model, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"50198584","metadata":{"id":"50198584","outputId":"e1d1dead-3d2c-4d09-ca36-2e0035cfc7c4"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["q_model.load_state_dict(torch.load(\"models/_fbgemm_quantized_model_dict.pth\"))"]},{"cell_type":"code","execution_count":null,"id":"2fbcf77a","metadata":{"id":"2fbcf77a","outputId":"31fbcf48-c544-4fe1-9fd5-f018115f54f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Quantized model test time: 56.59 seconds\n","Test Loss: 0.1984, Test Accuracy: 93.71%\n"]}],"source":["# evaluating quantized model on test set\n","q_test_accuracies = []\n","q_test_losses = []\n","q_batch_sizes = []\n","with torch.no_grad():\n","    test_time_start = time.time()\n","    for images, labels in test_loader:\n","        images = images.cpu()\n","        labels = labels.cpu()\n","        outputs = q_model(images)\n","        q_test_losses.append(loss_fn(outputs, labels).item())\n","        q_test_accuracies.append((outputs.argmax(dim=1) == labels).float().mean().item())\n","        q_batch_sizes.append(len(labels))\n","\n","quantized_model_test_time = time.time() - test_time_start\n","print(f\"Quantized model test time: {quantized_model_test_time:.2f} seconds\")\n","q_test_accuracies_weighted = [acc * size for acc, size in zip(q_test_accuracies, q_batch_sizes)]\n","q_test_losses_weighted = [loss * size for loss, size in zip(q_test_losses, q_batch_sizes)]\n","print(f'Test Loss: {sum(q_test_losses_weighted)/sum(q_batch_sizes):.4f}, Test Accuracy: {sum(q_test_accuracies_weighted)/sum(q_batch_sizes)*100:.2f}%')"]},{"cell_type":"markdown","id":"f4dcf886","metadata":{"id":"f4dcf886"},"source":["The reduction in accuracy is not big, just under 3%, while loss almost doubles. A final result can be obtained by quantizing the pruned and finetuned model. As a reminder, this model was ft'd for 2 epochs on the validation dataset"]},{"cell_type":"code","execution_count":null,"id":"75059692","metadata":{"id":"75059692","outputId":"ec3191d7-426b-4216-e22d-b929e2702c8c"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["q_pruned_model = QuantizedPlantClassifier()\n","q_pruned_model.load_state_dict(torch.load(\"models/pruned_finetuned_model.pth\"))"]},{"cell_type":"code","execution_count":null,"id":"2e670edb","metadata":{"id":"2e670edb"},"outputs":[],"source":["q_pruned_model.eval()\n","q_pruned_model = fuse_model(q_pruned_model)\n","q_pruned_model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")"]},{"cell_type":"code","execution_count":null,"id":"a7785691","metadata":{"id":"a7785691","outputId":"afb60aac-9c39-4091-8243-f360aa0d75ac"},"outputs":[{"data":{"text/plain":["QuantizedPlantClassifier(\n","  (quant): QuantStub(\n","    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","  )\n","  (features): Sequential(\n","    (0): Sequential(\n","      (conv0): Conv2d(\n","        3, 16, kernel_size=(3, 3), stride=(1, 1)\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (bn0): BatchNorm2d(\n","        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (1): Sequential(\n","      (conv1): Conv2d(\n","        16, 32, kernel_size=(3, 3), stride=(1, 1)\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (bn1): BatchNorm2d(\n","        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (relu1): ReLU(inplace=True)\n","      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (2): Sequential(\n","      (conv2): Conv2d(\n","        32, 64, kernel_size=(3, 3), stride=(1, 1)\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (bn2): BatchNorm2d(\n","        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (relu2): ReLU(inplace=True)\n","      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (3): Sequential(\n","      (conv3): Conv2d(\n","        64, 128, kernel_size=(3, 3), stride=(1, 1)\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (bn3): BatchNorm2d(\n","        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (relu3): ReLU(inplace=True)\n","      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (4): Sequential(\n","      (conv4): Conv2d(\n","        128, 256, kernel_size=(3, 3), stride=(1, 1)\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (bn4): BatchNorm2d(\n","        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (relu4): ReLU(inplace=True)\n","      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (5): Sequential(\n","      (conv5): Conv2d(\n","        256, 512, kernel_size=(3, 3), stride=(1, 1)\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (bn5): BatchNorm2d(\n","        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","      )\n","      (relu5): ReLU(inplace=True)\n","      (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): LinearReLU(\n","      (0): Linear(in_features=2048, out_features=256, bias=True)\n","      (1): ReLU(inplace=True)\n","      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","    )\n","    (2): Identity()\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(\n","      in_features=256, out_features=39, bias=True\n","      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n","    )\n","  )\n","  (dequant): DeQuantStub()\n",")"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["torch.quantization.prepare(q_pruned_model,inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"5b719535","metadata":{"id":"5b719535"},"outputs":[],"source":["# calibration done on val dataset, it was mostly not trained on it (2 out of 11 epochs), also cannot do this on test set...\n","with torch.no_grad():\n","    for images, labels in val_loader:\n","        images = images.to(\"cpu\")\n","        q_pruned_model(images)"]},{"cell_type":"code","execution_count":null,"id":"2619d07e","metadata":{"id":"2619d07e","outputId":"1899523f-1d0a-4a47-92eb-55d4a7a9fac8"},"outputs":[{"data":{"text/plain":["QuantizedPlantClassifier(\n","  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n","  (features): Sequential(\n","    (0): Sequential(\n","      (conv0): QuantizedConv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.03866381198167801, zero_point=49)\n","      (bn0): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (1): Sequential(\n","      (conv1): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.32406851649284363, zero_point=77)\n","      (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (2): Sequential(\n","      (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.4443858861923218, zero_point=75)\n","      (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (3): Sequential(\n","      (conv3): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.7447038292884827, zero_point=77)\n","      (bn3): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu3): ReLU(inplace=True)\n","      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (4): Sequential(\n","      (conv4): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.198468565940857, zero_point=77)\n","      (bn4): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu4): ReLU(inplace=True)\n","      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (5): Sequential(\n","      (conv5): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.4272515773773193, zero_point=77)\n","      (bn5): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu5): ReLU(inplace=True)\n","      (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): QuantizedLinearReLU(in_features=2048, out_features=256, scale=0.2204882800579071, zero_point=0, qscheme=torch.per_channel_affine)\n","    (2): Identity()\n","    (3): QuantizedDropout(p=0.5, inplace=False)\n","    (4): QuantizedLinear(in_features=256, out_features=39, scale=1.2777177095413208, zero_point=99, qscheme=torch.per_channel_affine)\n","  )\n","  (dequant): DeQuantize()\n",")"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["torch.quantization.convert(q_pruned_model, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"4970fbae","metadata":{"id":"4970fbae"},"outputs":[],"source":["#torch.save(q_pruned_model.state_dict(), './models/_fbgemm_quantized_pruned_model_dict.pth')"]},{"cell_type":"code","execution_count":null,"id":"632c3b77","metadata":{"id":"632c3b77","outputId":"aadb9f97-655a-405d-cdd3-c09e10a2e81e"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\user2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\ao\\quantization\\observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n","  warnings.warn(\n","C:\\Users\\user2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\ao\\quantization\\observer.py:1209: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n","  warnings.warn(\n"]},{"data":{"text/plain":["QuantizedPlantClassifier(\n","  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n","  (features): Sequential(\n","    (0): Sequential(\n","      (conv0): QuantizedConv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)\n","      (bn0): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu0): ReLU(inplace=True)\n","      (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (1): Sequential(\n","      (conv1): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)\n","      (bn1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu1): ReLU(inplace=True)\n","      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (2): Sequential(\n","      (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)\n","      (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu2): ReLU(inplace=True)\n","      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (3): Sequential(\n","      (conv3): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)\n","      (bn3): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu3): ReLU(inplace=True)\n","      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (4): Sequential(\n","      (conv4): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)\n","      (bn4): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu4): ReLU(inplace=True)\n","      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (5): Sequential(\n","      (conv5): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)\n","      (bn5): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu5): ReLU(inplace=True)\n","      (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): QuantizedLinear(in_features=2048, out_features=256, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n","    (2): ReLU(inplace=True)\n","    (3): QuantizedDropout(p=0.5, inplace=False)\n","    (4): QuantizedLinear(in_features=256, out_features=39, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n","  )\n","  (dequant): DeQuantize()\n",")"]},"execution_count":88,"metadata":{},"output_type":"execute_result"}],"source":["# LOADING QUANTIZED\n","q_pruned_model = QuantizedPlantClassifier()\n","q_pruned_model.cpu()\n","q_pruned_model.eval()\n","q_pruned_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n","torch.quantization.prepare(q_pruned_model, inplace=True)\n","torch.quantization.convert(q_pruned_model, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"9d3218f2","metadata":{"id":"9d3218f2","outputId":"d3b2137c-f497-4ef7-f26b-3abe4c2df41a"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["q_pruned_model.load_state_dict(torch.load(\"models/_fbgemm_quantized_pruned_model_dict.pth\"))"]},{"cell_type":"code","execution_count":null,"id":"b77d862b","metadata":{"id":"b77d862b","outputId":"db23451c-4227-43c1-c7a7-ff62375de279"},"outputs":[{"name":"stdout","output_type":"stream","text":["Quantized pruned model size in MBs: 2.0553064346313477\n"]}],"source":["#inspecting file size on disk\n","quantized_pruned_model_size = os.path.getsize('models/_fbgemm_quantized_pruned_model_dict.pth') / (1024 * 1024)  # size in MB\n","print(f'Quantized pruned model size in MBs: {quantized_pruned_model_size}')"]},{"cell_type":"markdown","id":"6e0defb4","metadata":{"id":"6e0defb4"},"source":["Reasonably, a similar reduction happens, a way to remove entries for 0-parameters would definitely decrease model file size to minimal dimensions"]},{"cell_type":"code","execution_count":null,"id":"16897eaf","metadata":{"id":"16897eaf","outputId":"0398663c-893b-4bdf-fb0f-49a5487d62a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Quantized pruned model test time: 57.11 seconds\n","Test Loss: 0.0938, Test Accuracy: 96.54%\n"]}],"source":["# evaluating quantized-pruned-best_model on test set\n","qp_test_accuracies = []\n","qp_test_losses = []\n","qp_batch_sizes = []\n","with torch.no_grad():\n","    test_time_start = time.time()\n","    for images, labels in test_loader:\n","        images = images.cpu()\n","        labels = labels.cpu()\n","        outputs = q_pruned_model(images)\n","        qp_test_losses.append(loss_fn(outputs, labels).item())\n","        qp_test_accuracies.append((outputs.argmax(dim=1) == labels).float().mean().item())\n","        qp_batch_sizes.append(len(labels))\n","quantized_pruned_model_test_time = time.time() - test_time_start\n","print(f\"Quantized pruned model test time: {quantized_pruned_model_test_time:.2f} seconds\")\n","qp_test_accuracies_weighted = [acc * size for acc, size in zip(qp_test_accuracies, qp_batch_sizes)]\n","qp_test_losses_weighted = [loss * size for loss, size in zip(qp_test_losses, qp_batch_sizes)]\n","print(f'Test Loss: {sum(qp_test_losses_weighted)/sum(qp_batch_sizes):.4f}, Test Accuracy: {sum(qp_test_accuracies_weighted)/sum(qp_batch_sizes)*100:.2f}%')"]},{"cell_type":"markdown","id":"ebf63c36","metadata":{"id":"ebf63c36"},"source":["The quantized-pruned(& finetuned) model achieves the best value so far for test loss, and obtains the same level of accuracy as the initial base model despite the 75% model size reduction"]},{"cell_type":"markdown","id":"23612cca","metadata":{"id":"23612cca"},"source":["### Speed comparison\n","\n","Since quantized models bring issues when using the gpu, the speed comparison will be done on the cpu for all models."]},{"cell_type":"code","execution_count":null,"id":"0771e8ce","metadata":{"id":"0771e8ce"},"outputs":[],"source":["# averaging inference times per image using dummy inputs\n","def test_inference_time(model, input_size=1000):\n","    loader = DataLoader(\n","        test_dataset,\n","        batch_size=input_size,\n","        shuffle=False\n","    )\n","    model.cpu()\n","    model.eval()\n","    start_time = time.time()\n","    with torch.no_grad():\n","        model(next(iter(loader))[0])\n","    end_time = time.time()\n","    total_time = end_time - start_time\n","    return total_time / input_size"]},{"cell_type":"code","execution_count":null,"id":"0304b82b","metadata":{"id":"0304b82b"},"outputs":[],"source":["model_speeds = {\n","    \"base\": 0.0,\n","    \"pruned\": 0.0,\n","    \"quantized\": 0.0,\n","    \"quantized_pruned\": 0.0\n","}"]},{"cell_type":"markdown","id":"a6d393de","metadata":{"id":"a6d393de"},"source":["Running cells independently to measure speed"]},{"cell_type":"code","execution_count":null,"id":"37b92408","metadata":{"id":"37b92408"},"outputs":[],"source":["model_speeds[\"base\"] = test_inference_time(model)  # for base model"]},{"cell_type":"code","execution_count":null,"id":"b3511548","metadata":{"id":"b3511548"},"outputs":[],"source":["model_speeds[\"pruned\"] = test_inference_time(pruned_model)  # for pruned model"]},{"cell_type":"code","execution_count":null,"id":"4c2c10b0","metadata":{"id":"4c2c10b0"},"outputs":[],"source":["model_speeds[\"quantized\"] = test_inference_time(q_model)  # for quantized model"]},{"cell_type":"code","execution_count":null,"id":"1ae54c6e","metadata":{"id":"1ae54c6e"},"outputs":[],"source":["model_speeds[\"quantized_pruned\"] = test_inference_time(q_pruned_model)  # for quantized pruned model"]},{"cell_type":"code","execution_count":null,"id":"0a111d7f","metadata":{"id":"0a111d7f","outputId":"09eb64e2-9020-4b8a-eb89-d73bf49e0f7b"},"outputs":[{"name":"stdout","output_type":"stream","text":["quantized_pruned model inference time per image: 10.065129757 ms\n","quantized model inference time per image: 13.045636892 ms\n","pruned model inference time per image: 15.727429628 ms\n","base model inference time per image: 23.225077152 ms\n"]}],"source":["# sort and display\n","sorted_speeds = dict(sorted(model_speeds.items(), key=lambda item: item[1]))\n","for model_type, speed in sorted_speeds.items():\n","    print(f\"{model_type} model inference time per image: {speed*1000:.9f} ms\")"]},{"cell_type":"markdown","id":"f3e47a7b","metadata":{"id":"f3e47a7b"},"source":["The quantized models achieve the fastest inference, the quantized_pruned achieves higher accuracy and lower loss so it is preferable. Also, the pruned model is much faster than the base model, so the contrbution of pruning is also clear"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}